---
title: "Homework 1"
author: "Zhengting (Johnathan) He"
date: "2022/2/3"
output:
  pdf_document: 
    latex_engine: lualatex
  html_document: default
  word_document: default
---
\fontsize{8}{6}
\fontseries{b}
\selectfont


```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE)
```


```{r, message = FALSE, results = "hide"}
# set up
require(tidyverse)
require(tidymodels)
require(discrim)
require(ISLR)
```


# Linear regression ISL: 3.7


## Problem 5


According to the information provided,


$$
\hat{y_i} = x_i * \hat{\beta} \\ = x_i * \frac{\sum_{i=1}^n{x_iy_i}}{\sum_{i'=1}^n{x_{i'}^2}} \\
= x_i * \frac{\sum_{i'=1}^n{x_i'y_i'}}{\sum_{j=1}^n{x_{j}^2}} \\
= \sum_{i'=1}^n{(\frac{x_i'*x_i}{\sum_{j=1}^n{x_{j}^2}}*y_i')}
$$


Thus,


$$
a_i' = \frac{x_i'*x_i}{\sum_{j=1}^n{x_{j}^2}}
$$


## Problem 6


According to $(3.4)$, the regression line given by least square approach - in a simple linear regression case - is:


$$
y = \hat{\beta_1}*x + \hat{\beta_0} = \hat{\beta_1}*x + (\bar{y} - \hat{\beta_1}*\bar{x})
$$


Which is equivalent to the line:


$$
\hat{\beta_1}*(x - \bar{x}) + (\bar{y} - y) = 0
$$


Since $\hat{\beta_1}*(\bar{x} - \bar{x}) + (\bar{y} - \bar{y}) = 0$, $(\bar{x}, \bar{y})$ is a solution to the above equation. In other words, the least square line always passes through the point $(\bar{x}, \bar{y})$.


## Problem 9


### (a)


```{r}
pairs(Auto[, 1:8], cex = 0.5)
```


### (b)


```{r}
Auto %>% 
    select(- name) %>% 
    corrr::correlate() %>% 
    knitr::kable(digits = 4)
```


```{r, fig.width = 4, fig.height = 3}
Auto %>% 
    select(- name) %>% 
    corrr::correlate() %>% 
    corrr::rplot(colours = c("indianred2", "black", "skyblue1")) +
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


### (c)


```{r}
linear_model <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression") %>% 
    fit(mpg ~ . - name, data = Auto)

linear_model %>% pluck("fit") %>% summary()
```


+ i. According to the global test on the null hypothesis $H_0: \beta_1 = \beta_2 = … = \beta_p = 0$, the $F-statistic = 252.4$ on 7 and 384 df, leading to a $p-value < 2.2e-16$. Thus, we reject the null hypothesis on a significance level of $\alpha = 0.05$ and conclude that there is a significant relationship between the predictors and the response.


+ ii. According to the t-value and associated p-value of the regression coefficients for each predictor, `displacement`, `weight`, `year`, and `origin` are statistically significantly associated with the response, under a significance level of $\alpha = 0.05$.


+ iii. The $\beta$ coefficient of `year` suggests that, with 1 unit increase in `year`, there are 0.75 unit increase in `mpg`, holding `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, and `origin` constant.


### (d)


```{r, fig.width = 6, fig.height = 4}
par(mfrow = c(2,2))
plot(linear_model %>% pluck("fit"))
```


+ The residuals vs. fitted plot with the associated lowess smooth curve show some non-linear pattern of the data, especially in the tail. The plot may suggest a polynomial regression, instead of a linear regression, may provide a better fit of the data. Some points, such as #323, #327, may be outliers and impact the estimation of the parameters (the fit of the line to the data).


+ The normal Q-Q plot shows, mostly, residuals follow a normal distribution, but in the right tail of the distribution there are some right skewness patterns, indicating the normal distribution of residuals assumption may be violated.


+ The scale-location plot shows the line is roughly horizontal, while the residuals show a slight funnel pattern, as their variation slightly increase as the fitted values increase, indicating the equal variance assumption may not hold.


+ The residuals vs leverage plot does not show any points outside the Cook's distance lines. However, point #14 still has a unusual high leverage relative to its standardized residual, indicating it may impact the estimation of the parameters (the fit of the line to the data).


### (e)


```{r}
linear_model <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression") %>% 
    fit(mpg ~ . - name + acceleration*year + horsepower*weight, data = Auto)

linear_model %>% pluck("fit") %>% summary()
```


```{r}
linear_model <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression") %>% 
    fit(mpg ~ . - name + acceleration:year + horsepower:weight, data = Auto)

linear_model %>% pluck("fit") %>% summary()
```


+ The above two models show identical output, however generally, the colon (:) is used to indicate an interaction between two or more variables in model formula; while the asterisk (*) is use to indicate all main effects and interactions among the variables that it joins. [^1]

[^1]: https://www.stat.berkeley.edu/~s133/Aov1a.html#:~:text=To%20express%20the%20idea%20of,the%20variables%20that%20it%20joins.


+ The interaction terms `acceleration:year` and `horsepower:weight` are both statistically significant under a significance level $\alpha = 0.05$. The model accuracy increased, as shown the increased $R^2$ of the interaction model compared to the original model.


### (f)


Exploration of predictor transformation:


```{r}
# apply log transformation for each predictor
names(Auto[2:8]) %>% 
    map_dfr(~ tibble(
        `log transformation` = .x,
        `r squared` = (lm(mpg ~ . - name - get(.x) + log(get(.x)), data = Auto)
                       %>% summary())$r.squared
    )) %>% 
    knitr::kable()
```


$log(X)$ transforming `horsepower` or `weight` improve model accuracy according to the $R^2$ values, compared to the original model which $R^2 = 0.8215$.


```{r}
# apply sqrt transformation for each predictor
names(Auto[2:8]) %>% 
    map_dfr(~ tibble(
        `sqrt transformation` = .x,
        `r squared` = (lm(mpg ~ . - name - get(.x) + sqrt(get(.x)), data = Auto)
                       %>% summary())$r.squared
    )) %>% 
    knitr::kable()
```


$\sqrt{X}$ transforming `displacement`, `horsepower`, or `weight` improve model accuracy according to the $R^2$ values, compared to the original model which $R^2 = 0.8215$.


```{r}
# apply polynomial transformation for each predictor
names(Auto[2:8]) %>% 
    map_dfr(~ tibble(
        `quadratic transformation` = .x,
        `r squared` = (lm(mpg ~ . - name - get(.x) + I(get(.x)^2), data = Auto)
                       %>% summary())$r.squared
    )) %>% 
    knitr::kable()
```


$X^2$ transforming `displacement`, `horsepower`, or `weight` improve model accuracy according to the $R^2$ values, compared to the original model which $R^2 = 0.8215$.


```{r}
# combine interaction terms and transformation
linear_model <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression") %>% 
    fit(mpg ~ . - name - horsepower - weight - displacement
        + acceleration:year + sqrt(horsepower)*sqrt(weight) + log(displacement),
        data = Auto)

linear_model %>% pluck("fit") %>% summary()
```


The above regression output shows that: combining interaction terms and transformation on predictors, the model accuracy increases as the model flexibility increases — the above model results in a $R^2 = 0.875$, compared to the original model which $R^2 = 0.8215$. Compared to the original model, the $\beta$ coefficients of more predictors reach statistical significance under $\alpha = 0.05$.


## Problem 15


```{r}
Boston <- MASS::Boston
```


### (a)


```{r}
names(Boston[2:ncol(Boston)]) %>% 
    map_dfr(~ tibble(
        `predictor` = .x,
        `Estimate` = (lm(crim ~ get(.x), data = Boston) %>%
                          summary())$coefficients[2,1] %>% round(3),
        `Std. Error` = (lm(crim ~ get(.x), data = Boston) %>%
                            summary())$coefficients[2,2] %>% round(3),
        `t value` = (lm(crim ~ get(.x), data = Boston) %>%
                         summary())$coefficients[2,3] %>% round(3),
        `Pr(>|t|)` = (lm(crim ~ get(.x), data = Boston) %>%
                          summary())$coefficients[2,4]
    )) %>% 
    mutate(`Significant` = ifelse(`Pr(>|t|)` < 0.05, "*", "")) %>% 
    knitr::kable()
```


```{r, fig.width = 7, fig.height = 5}
Boston %>% 
    pivot_longer(cols = -crim,
                 names_to = "predictor",
                 values_to = "value") %>% 
    ggplot(aes(x = value, y = crim)) +
    facet_wrap(~ predictor, scales = "free") +
    geom_point() + 
    geom_smooth(method = "lm") +
    xlab("predictor") + 
    theme_bw()
```


According to the regression output, all predictors except `chas` showed a significant association with the outcome `crim`, under significance level $\alpha = 0.05$.


### (b)


```{r}
linear_model <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression") %>% 
    fit(crim ~ ., data = Boston)

linear_model %>% pluck("fit") %>% summary()
```


According to the output above, global test rejects the null hypothesis that $H_0: \beta_1 = \beta_2 = … = \beta_p = 0$, and the resulting $R^2 = 0.454$.


Under a significance level of $\alpha = 0.05$, we can reject the null hypothesis $H_0: \beta_j = 0$ for the following predictors: `zn`, `dis`, `rad`, `black`, `medv`.


### (c)


```{r}
# data cleaning
single.coefficient <- names(Boston[2:ncol(Boston)]) %>% 
    map_dfr(~ tibble(
        `predictor` = .x,
        `single.estimate` = (lm(crim ~ get(.x), data = Boston) %>%
                          summary())$coefficients[2,1]
    ))

multiple.coefficient <- tibble(
    `predictor` = (linear_model %>% pluck("fit"))$coefficients %>% 
        names(),
    `multiple.estimate` = (linear_model %>% pluck("fit"))$coefficients
)

combine.coefficient <- single.coefficient %>% 
    left_join(multiple.coefficient, by = "predictor")
```


```{r, fig.width = 5, fig.height = 3}
# plotting the coefficients
combine.coefficient %>% 
    ggplot(aes(x = single.estimate, y = multiple.estimate)) + 
    geom_point() +
    xlab("Coefficient of Single Linear Regression Model") +
    ylab("Coefficient of Multiple Linear Regression Model") +
    theme_bw()
```


Compared to univariate regression model, multiple regression coefficients represent the change associated with each predictor holding all other predictors constant. Direction of some of the coefficients estimates changed compared to the univariate model, after adjusting for the value of other predictors.


### (d)


```{r}
# check class of each predictor
skimr::skim(Boston)
```


```{r}
# `chas` is a binary variable and should be removed from polynomial regression
names(Boston[2:ncol(Boston)])[- 3] %>% 
    map_dfr(~ tibble(
        `predictor` = .x,
        `p-value x` = (lm(crim ~ poly(get(.x), 3), data = Boston) %>% 
    summary())$coefficients[2,4],
        `p-value x^2` = (lm(crim ~ poly(get(.x), 3), data = Boston) %>% 
    summary())$coefficients[3,4],
        `p-value x^3` = (lm(crim ~ poly(get(.x), 3), data = Boston) %>% 
    summary())$coefficients[4,4])) %>% 
    mutate(`Significant x` = ifelse(`p-value x` < 0.05, "*", ""),
           `Significant x^2` = ifelse(`p-value x^2` < 0.05, "*", ""),
           `Significant x^3` = ifelse(`p-value x^3` < 0.05, "*", "")) %>% 
    select(predictor, `p-value x`, `Significant x`, `p-value x^2`,
           `Significant x^2`, `p-value x^3`, `Significant x^3`) %>% 
    knitr::kable()
```


According to the regression output above, for a significance level of $\alpha = 0.05$, the $\beta$ coefficients for all predictors are statistically significant on $X$ level. The $\beta$ coefficients for all predictors except `black` are statistically significant on $X^2$ level (quadratic). The $\beta$ coefficients for predictors `indus`, `nox`, `age`, `dis`, `ptratio`, `medv` are significant on $X^3$ level (cubic). Thus, there is evidence for a non-linear relationship for all predictors except `black`.


# Classification ISL: 4.7


## Problem 1


$$
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
$$


$$
p(X) + p(X)*e^{\beta_0 + \beta_1X} = e^{\beta_0 + \beta_1X}
$$


$$
p(X) = (1 - p(X)) * e^{\beta_0 + \beta_1X}
$$


$$
\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1X}
$$


## Problem 8


+ For model selection, we want to choose the method that gives the lowest test error rate, as opposed to the lowest training error rate.

+ For 1-nearest neighbors ($K = 1$) method, the nearest neighbor is the point itself,

  - This will result in a error rate = 0% in the training dataset.
  
  - Since the average error rate is 18%, the error rate in the testing dataset is 36%.

$$
Pr(Y = j | X = x_i) = \frac{1}{K}\sum_{i \in N_0}{I(y_i = j)} = I(y_i = j) = \left\{\begin{aligned}
1 \text{   , if } y_i = j \\
0 \text{   , if } y_i \not = j
\end{aligned}
\right.
$$
  
+ For logistic regression method, the error rate in the testing dataset is 30%.

+ Based on the smaller test error rate, we prefer logistic regression for classification of new observations.


## Problem 10


### (a)


```{r}
skimr::skim(Weekly)
```


```{r}
Weekly %>% 
    select(- Direction) %>% 
    corrr::correlate() %>% 
    knitr::kable(digits = 4)
```


```{r, fig.width = 4.5, fig.height = 2}
Weekly %>% 
    select(- Direction) %>% 
    corrr::correlate() %>% 
    corrr::rplot(colours = c("indianred2", "black", "skyblue1")) +
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


```{r}
pairs(Weekly[, 1:8], cex = 0.5)
```


As shown in the correlation matrix and pairwise scatterplot above, there is an increasing trend of number of shares traded on the previous day (`volumn`) over time (`year`) ($r = 0.84$). There are no apparent trend or correlation between the rest variables. Percentage returns for previous trading days (`Lag1`, `Lag2`, `Lag3`, `Lag4`, `Lag5`) are similar as the percentage return on the data in question (`today`), and the correlations are nearly zero. There are no apparent trend of percentage returns (`Lag1`, `Lag2`, `Lag3`, `Lag4`, `Lag5`, `today`) over time (`year`).


### (b)


```{r}
# fit logistic regression model
logistic_model <- logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification") %>% 
    fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly)
```


```{r}
logistic_model %>% 
    pluck("fit") %>% 
    summary()
```


Under a significance level of $\alpha = 0.05$, the only predictor appear to be statistically significant is `Lag2`.


### (c)


```{r}
# confusion matrix
augment(logistic_model, new_data = Weekly) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(logistic_model, new_data = Weekly) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


```{r}
# sensitivity
augment(logistic_model, new_data = Weekly) %>% 
    sens(truth = Direction, estimate = .pred_class, event_level = "second") %>% 
    knitr::kable()
```


```{r}
# specificity
augment(logistic_model, new_data = Weekly) %>% 
    spec(truth = Direction, estimate = .pred_class, event_level = "second") %>% 
    knitr::kable()
```


```{r}
# precision
augment(logistic_model, new_data = Weekly) %>% 
    precision(truth = Direction, estimate = .pred_class, event_level = "second") %>% 
    knitr::kable()
```



+ As shown by the output above, the overall fraction of correct predictions is 56.11%.

+ Considering class-specific fraction of correct predictions, the sensitivity (proportion of true "Up" days identified) is relatively high - 92.07%, well the specificity (proportion of true "Down" days identified) is rather low - 11.16%, indicating a large error rate in prediction of "Down" class.

+ Precision (positive predicted value, proportion of those predicted as "Up" direction that are truely "Up" direction) is 56.43%, just slightly better than random guessing due to change (proportion of "Up" days: 605/1089 = 55.6%), indicating the prediction precision is low.

+ Above results are the performance of the same training data, which will overestimate the accuracy in external data for testing.


### (d)


```{r}
# train and test split
Weekly_train <- Weekly %>% filter(Year %in% c(1990:2008))
Weekly_test <- Weekly %>% filter(Year %in% c(2009:2010))
```


```{r}
# fit logistic regression model
logistic_model <- logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification") %>% 
    fit(Direction ~ Lag2, data = Weekly_train)
```


```{r}
# confusion matrix
augment(logistic_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(logistic_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ The overall fraction of correct predictions for the held out data is 62.5%.


### (e)


```{r}
# fit linear discriminant analysis model
lda_model <- discrim_linear() %>%
    set_engine("MASS") %>% 
    set_mode("classification") %>%
    fit(Direction ~ Lag2, data = Weekly_train)
lda_model
```


```{r}
# confusion matrix
augment(lda_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(lda_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ The overall fraction of correct predictions for the held out data is 62.5%.


### (f)


```{r}
# fit quadratic discriminant analysis model
qda_model <- discrim_quad() %>% 
    set_engine("MASS") %>% 
    set_mode("classification") %>% 
    fit(Direction ~ Lag2, data = Weekly_train)
qda_model
```


```{r}
# confusion matrix
augment(qda_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(qda_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ The overall fraction of correct predictions for the held out data is 58.65%.


### (g)


**I noticed that {tidymodel} package only allows `kknn::kknn()` function for K nearest neighbors, which output may be different from `class::knn()` function.[^2] Below shows the results from `kknn::kknn()` function.**

[^2]: https://stackoverflow.com/questions/69955405/why-does-the-classknn-function-give-different-results-from-kknnkknn


```{r}
# fit K nearest neighbors model
knn_model <- nearest_neighbor(neighbors = 1) %>%
    set_mode("classification") %>%
    set_engine("kknn") %>% 
    fit(Direction ~ Lag2, data = Weekly_train)
knn_model
```


```{r}
# confusion matrix
augment(knn_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(knn_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ The overall fraction of correct predictions for the held out data is 50%.


**Below shows the results from `class::knn()` function.**


```{r}
require(class)
set.seed(1)
knn.prediction = knn(as.matrix(Weekly_train$Lag2), as.matrix(Weekly_test$Lag2), Weekly_train$Direction, k = 1)
table(knn.prediction, Weekly_test$Direction)
```


+ The overall fraction of correct predictions for the held out data is 50%.


### (h)


According to the confustion matrix and error rate in testing dataset, for predicting "Up" days, logistic regression and linear discriminant analysis provide the best results on this data, followed by quadratic discriminant analysis, while K nearest neighbors provide highest test error rates.


### (i)


```{r}
# logistic regression model, sqrt term of Lag1
logistic.m_model <- logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification") %>% 
    fit(Direction ~ Lag2 + sqrt(abs(Lag1)), data = Weekly_train)

augment(logistic.m_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
augment(logistic.m_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ Overall accuracy in testing data increased to 63.5% when including sqrt term of `Lag1`.


```{r}
# logistic regression model, quadratic term of Lag1
logistic.m_model <- logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification") %>% 
    fit(Direction ~ Lag2 + I(Lag1^2), data = Weekly_train)

augment(logistic.m_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
augment(logistic.m_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ Overall accuracy in testing data increased to 64.4% when including quadratic term of `Lag1`.


```{r}
# LDA, include all Lag as predictors
lda.m_model <- discrim_linear() %>%
    set_engine("MASS") %>% 
    set_mode("classification") %>%
    fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5, data = Weekly_train)

# confusion matrix
augment(lda.m_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(lda.m_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ No significant improvement on model accuracy when including all `Lag` as predictors.


```{r}
# LDA, interaction terms
lda.m_model <- discrim_linear() %>%
    set_engine("MASS") %>% 
    set_mode("classification") %>%
    fit(Direction ~ Lag2*Lag3, data = Weekly_train)

# confusion matrix
augment(lda.m_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(lda.m_model, new_data = Weekly_test) %>% 
    accuracy(truth = Direction, estimate = .pred_class) %>% 
    knitr::kable()
```


+ Overall accuracy in testing data increased to 62.5% when including interaction term of `Lag2` and `Lag3`.


```{r}
# apply K = 1:10 to KNN classifier, output confusion matrix and overall accuracy
KNN.K_model <- c(1:10) %>% 
    map(~ nearest_neighbor(neighbors = .x) %>%
        set_mode("classification") %>%
        set_engine("kknn") %>% 
        fit(Direction ~ Lag2, data = Weekly_train))

KNN.K_performance <- KNN.K_model %>% 
    map_dfr(~ tibble(
        `pred.down true.down` = (augment(.x, new_data = Weekly_test) %>% 
        conf_mat(truth = Direction, estimate = .pred_class))$table[1],
        `pred.up true.down` = (augment(.x, new_data = Weekly_test) %>% 
        conf_mat(truth = Direction, estimate = .pred_class))$table[2],
        `pred.down true.up` = (augment(.x, new_data = Weekly_test) %>% 
        conf_mat(truth = Direction, estimate = .pred_class))$table[3],
        `pred.up true.up` = (augment(.x, new_data = Weekly_test) %>% 
        conf_mat(truth = Direction, estimate = .pred_class))$table[4],
        `overall accuracy` = (augment(.x, new_data = Weekly_test) %>% 
        accuracy(truth = Direction, estimate = .pred_class))$.estimate
    ))

tibble(`K` = c(1:10)) %>% 
    bind_cols(KNN.K_performance) %>% 
    knitr::kable()
```


+ $K = 3$ or $K = 4$ provides the best results on the held out data - test accuracy of 55.77%. The confusion matrix is:

```{r}
KNN.K_model <- nearest_neighbor(neighbors = 3) %>%
    set_mode("classification") %>%
    set_engine("kknn") %>% 
    fit(Direction ~ Lag2, data = Weekly_train)

augment(KNN.K_model, new_data = Weekly_test) %>% 
    conf_mat(truth = Direction, estimate = .pred_class)
```


## Problem 11


### (a)


```{r}
# create binary indicator variable
Auto <- Auto %>% 
    mutate(mpg01 = ifelse(mpg > median(mpg, na.rm = TRUE), 1, 0))
```


### (b)


```{r}
# correlation matrix
Auto %>% 
    mutate(mpg01 = as.numeric(mpg01)) %>% 
    select(- mpg, - name) %>% 
    corrr::correlate() %>% 
    knitr::kable()
```


```{r}
# scatterplot
pairs(Auto[2:10], cex = 0.5)
```


```{r}
# code dependent variable as factor
Auto <- Auto %>% 
    mutate(mpg01 = recode_factor(mpg01, "0" = "0", "1" = "1"))
```


```{r, fig.width = 5, fig.height = 4}
# boxplot
Auto %>% 
    select(-mpg, -name) %>% 
    mutate(mpg01 = recode_factor(mpg01, "0" = "0", "1" = "1")) %>% 
    pivot_longer(cols = -mpg01,
                 names_to = "predictor",
                 values_to = "value") %>% 
    ggplot(aes(x = mpg01, y = value)) +
    geom_boxplot() +
    facet_wrap(~ predictor, scales = "free") +
    theme_bw()
```


According to the correlation matrix, scatterplot, and boxplot, there are negative relationships with relative high correlations between `cylinders`, `displacement`, `horsepower`, `weight` and `mpg01`. `acceleration`, `year`, `origin` and `mpg01` show some positive relationships, but the correlations are relatively low.


### (c)


```{r}
# split the training and testing on a 8:2 ratio, mpg01 as strata
set.seed(2022)
Auto_split <- initial_split(Auto, strata = mpg01, prop = 0.8)
Auto_train <- training(Auto_split)
Auto_test <- testing(Auto_split)
```


### (d)


```{r}
# fit linear discriminant analysis model
lda_model <- discrim_linear() %>%
    set_engine("MASS") %>% 
    set_mode("classification") %>%
    fit(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto_train)
lda_model
```


```{r}
# confusion matrix
augment(lda_model, new_data = Auto_test) %>% 
    conf_mat(truth = mpg01, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(lda_model, new_data = Auto_test) %>% 
    accuracy(truth = mpg01, estimate = .pred_class) %>% 
    knitr::kable()
```


The overall accuracy is 91.25%, indicating the total test error of the model obtained is 1 - 91.25% = 8.75%.


### (e)


```{r}
# fit quadratic discriminant analysis model
qda_model <- discrim_quad() %>% 
    set_engine("MASS") %>% 
    set_mode("classification") %>% 
    fit(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto_train)
qda_model
```


```{r}
# confusion matrix
augment(qda_model, new_data = Auto_test) %>% 
    conf_mat(truth = mpg01, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(qda_model, new_data = Auto_test) %>% 
    accuracy(truth = mpg01, estimate = .pred_class) %>% 
    knitr::kable()
```


The overall accuracy is 93.75%, indicating the total test error of the model obtained is 1 - 93.75% = 6.25%.


### (f)


```{r}
# fit logistic regression model
logistic_model <- logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification") %>% 
    fit(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto_train)
logistic_model %>% tidy()
```


```{r}
# confusion matrix
augment(logistic_model, new_data = Auto_test) %>% 
    conf_mat(truth = mpg01, estimate = .pred_class)
```


```{r}
# overall accuracy
augment(logistic_model, new_data = Auto_test) %>% 
    accuracy(truth = mpg01, estimate = .pred_class) %>% 
    knitr::kable()
```


The overall accuracy is 88.75%, indicating the total test error of the model obtained is 1 - 88.75% = 11.25%.


### (g)


```{r}
# apply K = 1:30 to KNN classifier, output confusion matrix and overall accuracy
KNN.K_model <- c(1:30) %>% 
    map(~ nearest_neighbor(neighbors = .x) %>%
        set_mode("classification") %>%
        set_engine("kknn") %>% 
        fit(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto_train))

KNN.K_performance <- KNN.K_model %>% 
    map_dfr(~ tibble(
        `pred.down true.down` = (augment(.x, new_data = Auto_test) %>% 
        conf_mat(truth = mpg01, estimate = .pred_class))$table[1],
        `pred.up true.down` = (augment(.x, new_data = Auto_test) %>% 
        conf_mat(truth = mpg01, estimate = .pred_class))$table[2],
        `pred.down true.up` = (augment(.x, new_data = Auto_test) %>% 
        conf_mat(truth = mpg01, estimate = .pred_class))$table[3],
        `pred.up true.up` = (augment(.x, new_data = Auto_test) %>% 
        conf_mat(truth = mpg01, estimate = .pred_class))$table[4],
        `overall accuracy` = (augment(.x, new_data = Auto_test) %>% 
        accuracy(truth = mpg01, estimate = .pred_class))$.estimate
    ))

tibble(`K` = c(1:30)) %>% 
    bind_cols(KNN.K_performance) %>% 
    mutate(`test errors` = 1 - `overall accuracy`) %>% 
    knitr::kable()
```


As shown in the table above, the lowest test error obtained is 6.25% when $K = 25$. $K = 25$ seems to perform the best on this data set. (As $K$ increase from 1 to 25, the associated test error increase; as $K$ increase from 25 to 30, there seems to be a diminish return that the associated test error becomes stable)


The confusion matrix and overall accuracy for KNN method when $K=25$ is outputted below:


```{r}
KNN_model <- nearest_neighbor(neighbors = 25) %>%
    set_mode("classification") %>%
    set_engine("kknn") %>% 
    fit(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto_train)

augment(KNN_model, new_data = Auto_test) %>% 
    conf_mat(truth = mpg01, estimate = .pred_class)
```


```{r}
augment(KNN_model, new_data = Auto_test) %>% 
    accuracy(truth = mpg01, estimate = .pred_class) %>% 
    knitr::kable()
```

