---
title: "Final project"
author: "Zhengting (Johnathan) He"
date: "2022/3/2"
output:
  rmdformats::robobook:
    highlight: kate
---


```{r "setup", include = FALSE}
require("knitr")
opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
opts_knit$set(root.dir = "D:/OneDrive - Johns Hopkins/Course/140.644.01 - Statistical Machine Learning Methods, Theory, and Applications/Assignments/biostats644-Assignments/Final project")
```


```{r}
# set up
setwd("D:/OneDrive - Johns Hopkins/Course/140.644.01 - Statistical Machine Learning Methods, Theory, and Applications/Assignments/biostats644-Assignments/Final project")

require(tidyverse)
require(data.table)
require(tidymodels)
require(probably)
require(vip)
```


```{r}
load("./DATA/nhanes2003-2004.Rda")
```


# 1. Preparation for modeling


## 1.1 Data cleaning


Since the goal of this project is to predict the mortality status for participants 50 years and older, I first drop participants age less than 50 years old according to the `RIDAGEYR` variable (Age at Screening Adjudicated). Then, I limit the data set to the outcome variable `mortstat`, and the list of informative predictors.


```{r}
data <- nhanes2003_2004 %>% 
    filter(as.numeric(as.character(RIDAGEYR)) >= 50) %>% 
    select(mortstat, RIDAGEYR, RIAGENDR, BPQ010, BPQ060, DIQ010, DIQ050, DIQ090, MCQ010, MCQ053,
           MCQ160A, MCQ160B, MCQ160K, MCQ160L, BMXWAIST, MCQ160M, MCQ220, MCQ245A, MCQ250A, MCQ250B,
           MCQ250C, MCQ250E, MCQ250F, MCQ250G, MCQ265, SSQ011, SSQ051, WHQ030, WHQ040, LBXRDW, HSD010,
           BPXPULS, BPXML1, VIQ200, BMXBMI, BPXSY1, BPXDI1)
```


I then check the class and distribution of all variables. All predictors are read in as categorical variables, which are not appropriate for predictors in continuous scale, including `RIDAGEYR`, `BMXWAIST`, `BPXML1`, `BPXSY1`, `BPXDI1`, `BMXBMI`, and `LBXRDW`, according to the codebook. The following data cleaning code aims to assign correct class for each variable, and change non-informative values (e.g., answer as "refused" or "don't know") to `NA`. I also add variable label for each variable, and add value labels for categorical variables.


```{r}
data <- data %>% 
    mutate(
        # outcome
        mortstat = factor(mortstat, levels = c(0, 1),
                                    labels = c("Assumed alive", "Assumed Deceased")),
        
        # demographic variables
        RIDAGEYR = as.numeric(as.character(RIDAGEYR)),
        RIAGENDR = factor(RIAGENDR, levels = c(1, 2), labels = c("Male", "Female")),
        
        # examination
        BMXWAIST = as.numeric(as.character(BMXWAIST)),
        BPXPULS = factor(BPXPULS, levels = c(1, 2), labels = c("Regular", "Irregular")),
        BPXML1 = as.numeric(as.character(BPXML1)),
        BPXSY1 = as.numeric(as.character(BPXSY1)),
        BPXDI1 = as.numeric(as.character(BPXDI1)),
        VIQ200 = ifelse(VIQ200 == 9, NA, VIQ200),
        VIQ200 = factor(VIQ200, levels = c(1, 2), labels = c("Yes", "No")),
        BMXBMI = as.numeric(as.character(BMXBMI)),
        
        # questionnaire
        across(c(BPQ010, BPQ060, DIQ010, DIQ050, DIQ090, MCQ010, MCQ053, MCQ160A, MCQ160B,
                 MCQ160K, MCQ160L, MCQ160M, MCQ220, MCQ245A, MCQ250A, MCQ250B, MCQ250C,
                 MCQ250E, MCQ250F, MCQ250G, MCQ265, SSQ011, SSQ051, WHQ030, WHQ040, HSD010),
               ~ ifelse(.x == 7 | .x == 9, as.factor(NA), .x)),
        across(c(BPQ060, DIQ050, DIQ090, MCQ010, MCQ053, MCQ160A, MCQ160B, MCQ160K, MCQ160L,
                 MCQ160M, MCQ220, MCQ245A, MCQ250A, MCQ250B, MCQ250C, MCQ250E, MCQ250F,
                 MCQ250G, MCQ265),
               ~ factor(.x, levels = c(1, 2),
                        labels = c("Yes", "No"))),
        BPQ010 = factor(BPQ010, levels = c(1, 2, 3, 4, 5),
                                labels = c("Less than 6 months ago", "6 months to 1 year ago",
                                           "More than 1 year to 2 years ago",
                                           "More than 2 years ago", "Never")),
        DIQ010 = factor(DIQ010, levels = c(1, 2, 3), labels = c("Yes", "No", "Borderline")),
        SSQ011 = factor(SSQ011, levels = c(1, 2, 3),
                                labels = c("Yes", "No", "SP doesn't need help")),
        SSQ051 = factor(SSQ051, levels = c(1, 2, 3),
                                labels = c("Yes", "No", "Offered help but wouldn't accept it")),
        WHQ030 = factor(WHQ030, levels = c(1, 2, 3),
                                labels = c("Overweight", "Underweight", "About the right weight")),
        WHQ040 = factor(WHQ040, levels = c(1, 2, 3),
                                labels = c("More", "Less", "Stay about the same")),
        HSD010 = factor(HSD010, levels = c(1, 2, 3, 4, 5),
                                labels = c("Excellent", "Very good", "Good", "Fair", "Poor")),
        
        # laboratory
        LBXRDW = as.numeric(as.character(LBXRDW))
        ) %>% 
    # demographic variables
    mutate_at(vars(RIDAGEYR), funs(setattr(., "label", "Age at Screening Adjudicated"))) %>% 
    mutate_at(vars(RIAGENDR), funs(setattr(., "label", "Gender"))) %>% 
    
    # examination
    mutate_at(vars(BMXWAIST), funs(setattr(., "label", "Waist Circumference (cm)"))) %>% 
    mutate_at(vars(BPXPULS), funs(setattr(., "label", "Pulse regular or irregular?"))) %>% 
    mutate_at(vars(BPXML1), funs(setattr(., "label", "MIL: maximum inflation levels (mm Hg)"))) %>% 
    mutate_at(vars(BPXSY1), funs(setattr(., "label", "Systolic: Blood pres (1st rdg) mm Hg"))) %>% 
    mutate_at(vars(BPXDI1), funs(setattr(., "label", "Diastolic: Blood pres (1st rdg) mm Hg"))) %>% 
    mutate_at(vars(VIQ200), funs(setattr(., "label", "Eye surgery for cataracts?"))) %>% 
    mutate_at(vars(BMXBMI), funs(setattr(., "label", "Body Mass Index (kg/m**2)"))) %>% 
    
    # questionnaire
    mutate_at(vars(BPQ010), funs(setattr(., "label", "Last blood pressure reading by doctor"))) %>% 
    mutate_at(vars(BPQ060), funs(setattr(., "label", "Ever had blood cholesterol checked"))) %>% 
    mutate_at(vars(DIQ010), funs(setattr(., "label", "Doctor told you have diabetes"))) %>% 
    mutate_at(vars(DIQ050), funs(setattr(., "label", "Taking insulin now"))) %>% 
    mutate_at(vars(DIQ090), funs(setattr(., "label", "Ulcer/sore not healed within 4 weeks"))) %>% 
    mutate_at(vars(MCQ010), funs(setattr(., "label", "Ever been told you have asthma"))) %>% 
    mutate_at(vars(MCQ053), funs(setattr(., "label", "Taking treatment for anemia/past 3 mos"))) %>% 
    mutate_at(vars(MCQ160A), funs(setattr(., "label", "Doctor ever said you had arthritis"))) %>% 
    mutate_at(vars(MCQ160B), funs(setattr(., "label", "Ever told had congestive heart failure"))) %>% 
    mutate_at(vars(MCQ160K), funs(setattr(., "label", "Ever told you had chronic bronchitis"))) %>% 
    mutate_at(vars(MCQ160L), funs(setattr(., "label", "Ever told you had any liver condition"))) %>% 
    mutate_at(vars(MCQ160M), funs(setattr(., "label", "Ever told you had a thyroid problem"))) %>% 
    mutate_at(vars(MCQ220), funs(setattr(., "label", "Ever told you had cancer or malignancy"))) %>% 
    mutate_at(vars(MCQ245A), funs(setattr(., "label", "Work days missed for illness/maternity"))) %>%
    mutate_at(vars(MCQ250A), funs(setattr(., "label", "Blood relatives have diabetes"))) %>%
    mutate_at(vars(MCQ250B), funs(setattr(., "label", "Blood relatives have Alzheimer's"))) %>%
    mutate_at(vars(MCQ250C), funs(setattr(., "label", "Blood relatives have asthma"))) %>%
    mutate_at(vars(MCQ250E), funs(setattr(., "label", "Blood relatives have osteoporosis"))) %>%
    mutate_at(vars(MCQ250F), funs(setattr(., "label", "Blood relatives w/hypertension/stroke"))) %>%
    mutate_at(vars(MCQ250G), funs(setattr(., "label", "Blood relatives w/hypertension/stroke"))) %>%
    mutate_at(vars(MCQ265), funs(setattr(., "label", "Blood relative have/had prostate cancer"))) %>%
    mutate_at(vars(SSQ011), funs(setattr(., "label", "Anyone to help with emotional support"))) %>%
    mutate_at(vars(SSQ051), funs(setattr(., "label", "Anyone to help with financial support"))) %>%
    mutate_at(vars(WHQ030), funs(setattr(., "label", "How do you consider your weight"))) %>%
    mutate_at(vars(WHQ040), funs(setattr(., "label", "Like to weigh more, less or same"))) %>%
    mutate_at(vars(HSD010), funs(setattr(., "label", "General health condition"))) %>%

    # laboratory
    mutate_at(vars(LBXRDW), funs(setattr(., "label", "Red cell distribution width (%)")))
```


After checking on the values and distributions of continuous variables, there are several observations with outside of normal physiological range values in variable `BPXDI1`. The following code drops these observations.


```{r}
data <- data %>% 
    mutate(BPXDI1 = ifelse(BPXDI1 <= 20, NA, BPXDI1))
```


I then drop observations with missing values in the outcome variable `mortstat` or in any predictors.


```{r}
data <- data %>% drop_na()
```


The following results show the summary statistics of final data set used in model building.


```{r}
skimr::skim(data)
```


## 1.2 Exploratory data analysis


Below shows the pairwise relationships of each continuous variables and the outcome variable `mortstat`.


```{r, fig.width = 10, fig.height = 10}
data %>% 
    select(mortstat, RIDAGEYR, BMXWAIST, BPXML1, BPXSY1, BPXDI1, BMXBMI, LBXRDW) %>% 
    mutate(mortstat = factor(mortstat, labels = c("Alive", "Deceased"))) %>% 
    GGally::ggpairs(aes(color = mortstat, alpha = 0.5)) +
    theme_minimal()
```


Below shows the relationships between each categorical variables and the outcome variable `mortstat`.


```{r, fig.width = 10, fig.height = 20}
data %>% 
    select(- RIDAGEYR, - BMXWAIST, - BPXML1, - BPXSY1, - BPXDI1, - BMXBMI, - LBXRDW) %>% 
    pivot_longer(cols = - mortstat) %>%
    ggplot(aes(y = value, fill = mortstat)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(x = NULL, y = NULL, fill = NULL) +
    theme_minimal()
```


Below shows the pairwise correlations of all predictors.


```{r}
data %>% 
    select(-mortstat) %>% 
    mutate(across(everything(),
                  ~ as.numeric(.x))) %>% 
    corrr::correlate() %>% 
    corrr::rplot(colours = c("indianred2", "black", "skyblue1")) +
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


## 1.3 Train and test split


I split the data set into training set and testing set using a 8:2 ratio stratified by mortality status, and create a 10 fold cross-validation data set on the training set.


```{r}
set.seed(123)
data_split <- initial_split(data, strata = "mortstat", prop = 8/10)

data_train <- training(data_split)
data_test <- testing(data_split)

data_fold <- vfold_cv(data_train, strata = "mortstat", v = 10)
```


# 2. Generalized linear model


## 2.1 Logistic regression


### 2.1.1 Fit logistic regression model


I first specify a logistic regression model. Predictors which are closely correlated ($r > 0.8$) are dropped, and all else predictors are used in this model.


```{r}
# specify recipe, exclude variables closely correlated (r = 0.8)
logistic_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric()) %>% 
    step_corr(threshold = 0.8)
```


```{r}
# specify logistic model
logistic_spec <- logistic_reg() %>%
    set_engine("glm") %>% 
    set_mode("classification")
```


```{r}
# build logistic model workflow
logistic_workflow <- 
    workflow() %>% 
    add_model(logistic_spec) %>% 
    add_recipe(logistic_recipe)
```


I then fit the model using the training set.


```{r}
logistic_fit <- logistic_workflow %>%
    fit(data = data_train)
```


Below shows the coefficients and statistics associated with the fitted model.


```{r}
logistic_fit %>% tidy() %>% kable()
```


```{r}
glance(logistic_fit) %>% kable()
```


### 2.1.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
augment(logistic_fit, new_data = data_test) %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
augment(logistic_fit, new_data = data_test) %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(augment(logistic_fit, new_data = data_test) %>% 
                  sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(augment(logistic_fit, new_data = data_test) %>% 
                  spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(augment(logistic_fit, new_data = data_test) %>% 
                  roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an accuracy of 82.5% and an AUC of 0.843, a relative high sensitivity of 93.3%, but a low specificity of 39.6%.


### 2.1.3 Model post-processing


The results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold. The best threshold is selected based on a higher value in the $\text{j_index}$:[^1]

$$
\text{j_index} = \text{sens} + \text{spec} âˆ’ 1
$$

[^1]: Davis Vaughan. Where does probably fit in? URL: https://probably.tidymodels.org/articles/where-to-use.html.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- logistic_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest $\text{j_index}$.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and $\text{j_index}$ across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold)
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- logistic_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, after balancing sensitivity and specificity, the model results in an accuracy of 75.7%, a sensitivity of 74.8%, a specificity of 79.2%, and an AUC of 0.843.


## 2.2 Logistic regression with feature selection penalty (lasso regression)


### 2.2.1 Fit lasso regression model


I further fit the logistic regression by adding a feature selection penalty using lasso regression, to address over-fitting issues, especially considering many predictors are taken into account. All predictors are normalized before fitting the model.


```{r}
# specify recipe
lasso_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric()) %>% 
    step_normalize(all_predictors())
```


```{r}
# specify lasso model
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>% 
    set_engine("glmnet") %>% 
    set_mode("classification")
```


```{r}
# build lasso model workflow
lasso_workflow <- 
    workflow() %>% 
    add_model(lasso_spec) %>% 
    add_recipe(lasso_recipe)
```


```{r}
# tuning parameter
penalty_grid <- grid_regular(penalty(range = c(-4, 2)), levels = 50)
```


The best tuning parameter $\lambda$ is chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
set.seed(234)
tune_res <- lasso_workflow %>% 
    tune_grid(
    resamples = data_fold, 
    grid = penalty_grid,
    control = control_grid(save_pred = TRUE)
)

autoplot(tune_res) + theme_minimal()
```


```{r}
best_penalty <- select_best(tune_res, metric = "roc_auc")
best_penalty
```


I then re-fit the lasso regression model using the whole training set.


```{r}
lasso_final <- finalize_workflow(lasso_workflow, best_penalty)

lasso_final_fit <- fit(lasso_final, data = data_train)
```


Below shows the lasso selected predictors and its coefficients.

```{r}
lasso_final_fit %>%
    tidy() %>%
    filter(estimate != 0) %>%
    kable()
```


### 2.2.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_laaso_preds <- augment(lasso_final_fit, new_data = data_test)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_laaso_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_laaso_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_laaso_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_laaso_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an accuracy of 83.7% and an AUC of 0.846, a relative high sensitivity of 96.2%, but a low specificity of 34.0%.


### 2.2.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- lasso_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest $\text{j_index}$.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and $\text{j_index}$ across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold)
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- lasso_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, after balancing sensitivity and specificity, the model results in an accuracy of 80.2%, a sensitivity of 82.4%, a specificity of 71.7%, and an AUC of 0.846.


# 3. Tree-Based Methods


## 3.1 Random Forests


### 3.1.1 Fit random forests model


I further fit a random forests model to predict mortality status. The predictor numbers used in the model is $\sqrt{37-1}=6$.


```{r}
# specify recipe
rf_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric())
```


```{r}
# specify random forests model
rf_spec <- rand_forest(mtry = 6, trees = 1000, min_n = tune()) %>%
    set_engine("ranger") %>%
    set_mode("classification")
```


```{r}
# build random forests model workflow
rf_workflow <- workflow() %>% 
    add_model(rf_spec) %>% 
    add_recipe(rf_recipe)
```


The best tuning parameter $min_n$ (the number of observations needed to keep splitting nodes) is chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
# tuning parameter
set.seed(345)
tune_res <- tune_grid(
    rf_workflow,
    resamples = data_fold,
    grid = 20
)

tune_res %>% autoplot() + theme_minimal()
```


```{r}
best_min_n <- select_best(tune_res, "roc_auc")
best_min_n
```


I then re-fit the random forests model using the whole training set.


```{r}
rf_final <- finalize_workflow(rf_workflow, best_min_n)

rf_final_fit <- rf_final %>% fit(data = data_train)
rf_final_fit
```


Below shows the variance importance plot of predictors.


```{r}
rf_final <- finalize_model(rf_spec, best_min_n)
rf_final %>%
    set_engine("ranger", importance = "permutation") %>%
    fit(mortstat ~ ., data = data_train) %>%
    vip(geom = "col") +
    theme_minimal()
```


### 3.1.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_preds <- rf_final_fit %>%
    augment(new_data = data_test)
```


```{r}
data_preds %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an accuracy of 82.9% and an AUC of 0.864, a relative high sensitivity of 95.7%, but a low specificity of 32.1%.


### 3.1.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3 and 2.2.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- rf_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest $\text{j_index}$.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and $\text{j_index}$ across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold)
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- rf_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, after balancing sensitivity and specificity, the model results in an accuracy of 81.7%, a sensitivity of 84.3%, a specificity of 71.7%, and an AUC of 0.864.


## 3.2 Boosting trees


### 3.2.1 Fit boosting trees model


I further try boosting trees model to increase the prediction performance.


```{r}
# specify recipe
xgb_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric())
```


```{r}
# specify boosting trees model
xgb_spec <- boost_tree(trees = 1000, 
                       tree_depth = tune(), min_n = tune(), 
                       loss_reduction = tune(),
                       sample_size = tune(), mtry = tune(),
                       learn_rate = tune()) %>% 
    set_engine("xgboost") %>% 
    set_mode("classification")
```


```{r}
# build boosting trees workflow
xgb_wf <- workflow() %>%
    add_formula(mortstat ~ .) %>%
    add_model(xgb_spec)
```


Below code specify range of possible values for tuning parameters.


```{r}
# tuning parameters
xgb_grid <- grid_latin_hypercube(tree_depth(),
                                 min_n(),
                                 loss_reduction(),
                                 sample_size = sample_prop(),
                                 finalize(mtry(), data_train),
                                 learn_rate(),
                                 size = 30)
```


The best tuning parameters are chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
doParallel::registerDoParallel()

set.seed(456)
xgb_res <- tune_grid(xgb_workflow,
                     resamples = data_fold,
                     grid = xgb_grid,
                     control = control_grid(save_pred = TRUE))

xgb_res %>% autoplot() + theme_minimal()
```


```{r}
best_auc <- select_best(xgb_res, "roc_auc")
best_auc %>% kable()
```


I then re-fit the boosting trees model using the whole training set.


```{r}
xgb_final <- finalize_workflow(xgb_workflow, best_auc)

xgb_final_fit <- fit(xgb_final, data = data_train)
xgb_final_fit
```


Below shows the variance importance plot of predictors.


```{r}
xgb_final <- finalize_model(xgb_spec, best_auc)
xgb_final %>%
    set_engine("xgboost") %>%
    fit(mortstat ~ ., data = data_train) %>%
    vip(geom = "col") +
    theme_minimal()
```


### 3.2.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_preds <- xgb_final_fit %>%
    augment(new_data = data_test)
```


```{r}
data_preds %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an accuracy of 84.0% and an AUC of 0.858, a relative high sensitivity of 96.2%, but a low specificity of 35.8%.


### 3.2.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3, 2.2.3, and 3.1.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- xgb_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest $\text{j_index}$.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and $\text{j_index}$ across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold)
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- xgb_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, after balancing sensitivity and specificity, the model results in an accuracy of 79.1%, a sensitivity of 79.0%, a specificity of 79.2%, and an AUC of 0.858.
