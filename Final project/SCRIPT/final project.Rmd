---
title: "Final project"
author: "Zhengting (Johnathan) He"
date: "2022/3/2"
output:
  rmdformats::robobook:
    highlight: kate
---


```{r "setup", include = FALSE}
require("knitr")
opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
opts_knit$set(root.dir = "D:/OneDrive - Johns Hopkins/Course/140.644.01 - Statistical Machine Learning Methods, Theory, and Applications/Assignments/biostats644-Assignments/Final project")
```


```{r}
# set up
setwd("D:/OneDrive - Johns Hopkins/Course/140.644.01 - Statistical Machine Learning Methods, Theory, and Applications/Assignments/biostats644-Assignments/Final project")

require(tidyverse)
require(data.table)
require(tidymodels)
require(probably)
require(vip)
```


```{r}
load("./DATA/nhanes2003-2004.Rda")
```


# Content


In this final project, I first read in the data set, clean all informative predictors and the outcome variable, and conduct exploratory data analysis. I then do the training and testing set split, and create a 10-fold cross-validation data set using the training set.


In further sections, I presented the prediction results using the following models:


|Models|Engine|
|:--:|:--:|
|Logistic Regression|{glm}|
|Logistic Regression + Feature Selection (Lasso)|{glmnet}|
|Random Forests|{ranger}|
|Boosting Trees|{xgboost}|
|Support Vector Machines|{kernlab}|


All predictions are conducted using the {tidymodels} framework, and based on the following steps:

+ **Fit model**: specify recipe & pre-processing, specify model, build workflow, select tuning parameters based on cross-validation as needed, fit the finalized workflow using the full training set.

+ **Evaluate model performance**: estimate model performance using the testing set, threshold for classification is a predicted probability higher than 50%. Report accuracy, sensitivity, specificity, and area under the ROC curve.

+ **Model post-processing**: balance sensitivity and specificity using different predicted probability threshold, and report the above metrics associated with the most balanced threshold.


In the final section, I compare the predictive performance of all models used, and discuss the results.


# 1. Preparation for modeling


## 1.1 Data cleaning


Since the goal of this project is to predict the mortality status for participants 50 years and older, I first **drop participants age less than 50 years old** according to the `RIDAGEYR` variable (Age at Screening Adjudicated). Then, I **limit the data set to the outcome variable `mortstat`, and the list of informative predictors**.


```{r}
data <- nhanes2003_2004 %>% 
    filter(as.numeric(as.character(RIDAGEYR)) >= 50) %>% 
    select(mortstat, RIDAGEYR, RIAGENDR, BPQ010, BPQ060, DIQ010, DIQ050, DIQ090, MCQ010, MCQ053,
           MCQ160A, MCQ160B, MCQ160K, MCQ160L, BMXWAIST, MCQ160M, MCQ220, MCQ245A, MCQ250A, MCQ250B,
           MCQ250C, MCQ250E, MCQ250F, MCQ250G, MCQ265, SSQ011, SSQ051, WHQ030, WHQ040, LBXRDW, HSD010,
           BPXPULS, BPXML1, VIQ200, BMXBMI, BPXSY1, BPXDI1)
```


I then check the class and distribution of all variables. All predictors are read in as categorical variables, which are not appropriate for predictors in continuous scale, including `RIDAGEYR`, `BMXWAIST`, `BPXML1`, `BPXSY1`, `BPXDI1`, `BMXBMI`, and `LBXRDW`, according to the codebook. The following data cleaning code aims to **assign correct class for each variable**, and **change non-informative values** (e.g., answer as "refused" or "don't know") **to `NA`**. I also add variable label for each variable, and add value labels for categorical variables.


```{r}
data <- data %>% 
    mutate(
        # outcome
        mortstat = factor(mortstat, levels = c(0, 1),
                                    labels = c("Assumed alive", "Assumed Deceased")),
        
        # demographic variables
        RIDAGEYR = as.numeric(as.character(RIDAGEYR)),
        RIAGENDR = factor(RIAGENDR, levels = c(1, 2), labels = c("Male", "Female")),
        
        # examination
        BMXWAIST = as.numeric(as.character(BMXWAIST)),
        BPXPULS = factor(BPXPULS, levels = c(1, 2), labels = c("Regular", "Irregular")),
        BPXML1 = as.numeric(as.character(BPXML1)),
        BPXSY1 = as.numeric(as.character(BPXSY1)),
        BPXDI1 = as.numeric(as.character(BPXDI1)),
        VIQ200 = ifelse(VIQ200 == 9, NA, VIQ200),
        VIQ200 = factor(VIQ200, levels = c(1, 2), labels = c("Yes", "No")),
        BMXBMI = as.numeric(as.character(BMXBMI)),
        
        # questionnaire
        across(c(BPQ010, BPQ060, DIQ010, DIQ050, DIQ090, MCQ010, MCQ053, MCQ160A, MCQ160B,
                 MCQ160K, MCQ160L, MCQ160M, MCQ220, MCQ245A, MCQ250A, MCQ250B, MCQ250C,
                 MCQ250E, MCQ250F, MCQ250G, MCQ265, SSQ011, SSQ051, WHQ030, WHQ040, HSD010),
               ~ ifelse(.x == 7 | .x == 9, as.factor(NA), .x)),
        across(c(BPQ060, DIQ050, DIQ090, MCQ010, MCQ053, MCQ160A, MCQ160B, MCQ160K, MCQ160L,
                 MCQ160M, MCQ220, MCQ245A, MCQ250A, MCQ250B, MCQ250C, MCQ250E, MCQ250F,
                 MCQ250G, MCQ265),
               ~ factor(.x, levels = c(1, 2),
                        labels = c("Yes", "No"))),
        BPQ010 = factor(BPQ010, levels = c(1, 2, 3, 4, 5),
                                labels = c("Less than 6 months ago", "6 months to 1 year ago",
                                           "More than 1 year to 2 years ago",
                                           "More than 2 years ago", "Never")),
        DIQ010 = factor(DIQ010, levels = c(1, 2, 3), labels = c("Yes", "No", "Borderline")),
        SSQ011 = factor(SSQ011, levels = c(1, 2, 3),
                                labels = c("Yes", "No", "SP doesn't need help")),
        SSQ051 = factor(SSQ051, levels = c(1, 2, 3),
                                labels = c("Yes", "No", "Offered help but wouldn't accept it")),
        WHQ030 = factor(WHQ030, levels = c(1, 2, 3),
                                labels = c("Overweight", "Underweight", "About the right weight")),
        WHQ040 = factor(WHQ040, levels = c(1, 2, 3),
                                labels = c("More", "Less", "Stay about the same")),
        HSD010 = factor(HSD010, levels = c(1, 2, 3, 4, 5),
                                labels = c("Excellent", "Very good", "Good", "Fair", "Poor")),
        
        # laboratory
        LBXRDW = as.numeric(as.character(LBXRDW))
        ) %>% 
    # demographic variables
    mutate_at(vars(RIDAGEYR), funs(setattr(., "label", "Age at Screening Adjudicated"))) %>% 
    mutate_at(vars(RIAGENDR), funs(setattr(., "label", "Gender"))) %>% 
    
    # examination
    mutate_at(vars(BMXWAIST), funs(setattr(., "label", "Waist Circumference (cm)"))) %>% 
    mutate_at(vars(BPXPULS), funs(setattr(., "label", "Pulse regular or irregular?"))) %>% 
    mutate_at(vars(BPXML1), funs(setattr(., "label", "MIL: maximum inflation levels (mm Hg)"))) %>% 
    mutate_at(vars(BPXSY1), funs(setattr(., "label", "Systolic: Blood pres (1st rdg) mm Hg"))) %>% 
    mutate_at(vars(BPXDI1), funs(setattr(., "label", "Diastolic: Blood pres (1st rdg) mm Hg"))) %>% 
    mutate_at(vars(VIQ200), funs(setattr(., "label", "Eye surgery for cataracts?"))) %>% 
    mutate_at(vars(BMXBMI), funs(setattr(., "label", "Body Mass Index (kg/m**2)"))) %>% 
    
    # questionnaire
    mutate_at(vars(BPQ010), funs(setattr(., "label", "Last blood pressure reading by doctor"))) %>% 
    mutate_at(vars(BPQ060), funs(setattr(., "label", "Ever had blood cholesterol checked"))) %>% 
    mutate_at(vars(DIQ010), funs(setattr(., "label", "Doctor told you have diabetes"))) %>% 
    mutate_at(vars(DIQ050), funs(setattr(., "label", "Taking insulin now"))) %>% 
    mutate_at(vars(DIQ090), funs(setattr(., "label", "Ulcer/sore not healed within 4 weeks"))) %>% 
    mutate_at(vars(MCQ010), funs(setattr(., "label", "Ever been told you have asthma"))) %>% 
    mutate_at(vars(MCQ053), funs(setattr(., "label", "Taking treatment for anemia/past 3 mos"))) %>% 
    mutate_at(vars(MCQ160A), funs(setattr(., "label", "Doctor ever said you had arthritis"))) %>% 
    mutate_at(vars(MCQ160B), funs(setattr(., "label", "Ever told had congestive heart failure"))) %>% 
    mutate_at(vars(MCQ160K), funs(setattr(., "label", "Ever told you had chronic bronchitis"))) %>% 
    mutate_at(vars(MCQ160L), funs(setattr(., "label", "Ever told you had any liver condition"))) %>% 
    mutate_at(vars(MCQ160M), funs(setattr(., "label", "Ever told you had a thyroid problem"))) %>% 
    mutate_at(vars(MCQ220), funs(setattr(., "label", "Ever told you had cancer or malignancy"))) %>% 
    mutate_at(vars(MCQ245A), funs(setattr(., "label", "Work days missed for illness/maternity"))) %>%
    mutate_at(vars(MCQ250A), funs(setattr(., "label", "Blood relatives have diabetes"))) %>%
    mutate_at(vars(MCQ250B), funs(setattr(., "label", "Blood relatives have Alzheimer's"))) %>%
    mutate_at(vars(MCQ250C), funs(setattr(., "label", "Blood relatives have asthma"))) %>%
    mutate_at(vars(MCQ250E), funs(setattr(., "label", "Blood relatives have osteoporosis"))) %>%
    mutate_at(vars(MCQ250F), funs(setattr(., "label", "Blood relatives w/hypertension/stroke"))) %>%
    mutate_at(vars(MCQ250G), funs(setattr(., "label", "Blood relatives w/hypertension/stroke"))) %>%
    mutate_at(vars(MCQ265), funs(setattr(., "label", "Blood relative have/had prostate cancer"))) %>%
    mutate_at(vars(SSQ011), funs(setattr(., "label", "Anyone to help with emotional support"))) %>%
    mutate_at(vars(SSQ051), funs(setattr(., "label", "Anyone to help with financial support"))) %>%
    mutate_at(vars(WHQ030), funs(setattr(., "label", "How do you consider your weight"))) %>%
    mutate_at(vars(WHQ040), funs(setattr(., "label", "Like to weigh more, less or same"))) %>%
    mutate_at(vars(HSD010), funs(setattr(., "label", "General health condition"))) %>%

    # laboratory
    mutate_at(vars(LBXRDW), funs(setattr(., "label", "Red cell distribution width (%)")))
```


After checking on the values and distributions of continuous variables, there are several observations with outside of normal physiological range values in variable `BPXDI1`. The following code drops these observations.


```{r}
data <- data %>% 
    mutate(BPXDI1 = ifelse(BPXDI1 <= 20, NA, BPXDI1))
```


I then **drop observations with missing values** in the outcome variable `mortstat` or in any predictors.


```{r}
data <- data %>% drop_na()
```


The following results show the summary statistics of final data set used in model building.


```{r}
skimr::skim(data)
```


## 1.2 Exploratory data analysis


Below shows the pairwise relationships of each continuous variables and the outcome variable `mortstat`.


```{r, fig.width = 10, fig.height = 10}
data %>% 
    select(mortstat, RIDAGEYR, BMXWAIST, BPXML1, BPXSY1, BPXDI1, BMXBMI, LBXRDW) %>% 
    mutate(mortstat = factor(mortstat, labels = c("Alive", "Deceased"))) %>% 
    GGally::ggpairs(aes(color = mortstat, alpha = 0.5)) +
    theme_minimal()
```


Below shows the relationships between each categorical variables and the outcome variable `mortstat`.


```{r, fig.width = 10, fig.height = 20}
data %>% 
    select(- RIDAGEYR, - BMXWAIST, - BPXML1, - BPXSY1, - BPXDI1, - BMXBMI, - LBXRDW) %>% 
    pivot_longer(cols = - mortstat) %>%
    ggplot(aes(y = value, fill = mortstat)) +
    geom_bar(position = "fill") +
    facet_wrap(vars(name), scales = "free", ncol = 2) +
    labs(x = NULL, y = NULL, fill = NULL) +
    theme_minimal()
```


Below shows the pairwise correlations of all predictors.


```{r}
data %>% 
    select(-mortstat) %>% 
    mutate(across(everything(),
                  ~ as.numeric(.x))) %>% 
    corrr::correlate() %>% 
    corrr::rplot(colours = c("indianred2", "black", "skyblue1")) +
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


## 1.3 Train and test split


I **split the data set into training set and testing set using a 8:2 ratio stratified by mortality status**, and create a **10 fold cross-validation data set** on the training set.


```{r}
set.seed(123)
data_split <- initial_split(data, strata = "mortstat", prop = 8/10)

data_train <- training(data_split)
data_test <- testing(data_split)

data_fold <- vfold_cv(data_train, strata = "mortstat", v = 10)
```


# 2. Generalized linear model


## 2.1 Logistic regression


### 2.1.1 Fit logistic regression model


I first specify a logistic regression model. Predictors which are closely correlated (r > 0.8) are dropped, and all else predictors are used in this model.


```{r}
# specify recipe, exclude variables closely correlated (r = 0.8)
logistic_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric()) %>% 
    step_corr(threshold = 0.8)
```


```{r}
# specify logistic model
logistic_spec <- logistic_reg() %>%
    set_engine("glm") %>% 
    set_mode("classification")
```


```{r}
# build logistic model workflow
logistic_workflow <- 
    workflow() %>% 
    add_model(logistic_spec) %>% 
    add_recipe(logistic_recipe)
```


I then fit the model using the training set.


```{r}
logistic_fit <- logistic_workflow %>%
    fit(data = data_train)
```


Below shows the coefficients and statistics associated with the fitted model.


```{r}
logistic_fit %>% tidy() %>% kable()
```


```{r}
glance(logistic_fit) %>% kable()
```


### 2.1.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
augment(logistic_fit, new_data = data_test) %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
augment(logistic_fit, new_data = data_test) %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(augment(logistic_fit, new_data = data_test) %>% 
                  sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(augment(logistic_fit, new_data = data_test) %>% 
                  spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(augment(logistic_fit, new_data = data_test) %>% 
                  roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an **accuracy of 82.5%** and an **AUC of 0.843**, a relative high **sensitivity of 93.3%**, but a low **specificity of 39.6%**.


### 2.1.3 Model post-processing


The results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold. The best threshold is selected based on a higher value in the j_index:[^1]

j_index = sensitivity + specificity − 1

[^1]: Davis Vaughan. Where does probably fit in? URL: https://probably.tidymodels.org/articles/where-to-use.html.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- logistic_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest j_index.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and j_index across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold) %>% 
    kable()
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- logistic_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
data_preds_logistic <- data_preds_final
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, **after balancing sensitivity and specificity**, the model results in **an accuracy of 75.7%, a sensitivity of 74.8%, a specificity of 79.2%, and an AUC of 0.843**.


## 2.2 Logistic regression with feature selection penalty (lasso regression)


### 2.2.1 Fit lasso regression model


I further fit the logistic regression by adding a feature selection penalty using lasso regression, to address over-fitting issues, especially considering many predictors are taken into account. All predictors are normalized before fitting the model.


```{r}
# specify recipe
lasso_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric()) %>% 
    step_normalize(all_predictors())
```


```{r}
# specify lasso model
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>% 
    set_engine("glmnet") %>% 
    set_mode("classification")
```


```{r}
# build lasso model workflow
lasso_workflow <- 
    workflow() %>% 
    add_model(lasso_spec) %>% 
    add_recipe(lasso_recipe)
```


```{r}
# tuning parameter
penalty_grid <- grid_regular(penalty(range = c(-4, 2)), levels = 50)
```


The best tuning parameter `lambda` is chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
set.seed(234)
tune_res <- lasso_workflow %>% 
    tune_grid(
    resamples = data_fold, 
    grid = penalty_grid,
    control = control_grid(save_pred = TRUE)
)

autoplot(tune_res) + theme_minimal()
```


```{r}
best_penalty <- select_best(tune_res, metric = "roc_auc")
best_penalty %>% kable()
```


I then re-fit the lasso regression model using the whole training set.


```{r}
lasso_final <- finalize_workflow(lasso_workflow, best_penalty)

lasso_final_fit <- fit(lasso_final, data = data_train)
```


Below shows the lasso selected predictors and its coefficients.

```{r}
lasso_final_fit %>%
    tidy() %>%
    filter(estimate != 0) %>%
    kable()
```


### 2.2.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_laaso_preds <- augment(lasso_final_fit, new_data = data_test)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_laaso_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_laaso_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_laaso_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_laaso_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an **accuracy of 83.7%** and an **AUC of 0.846**, a relative high **sensitivity of 96.2%**, but a low **specificity of 34.0%**.


### 2.2.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- lasso_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest j_index$.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and j_index across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold) %>% 
    kable()
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- lasso_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
data_preds_lasso <- data_preds_final
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, **after balancing sensitivity and specificity**, the model results in **an accuracy of 80.2%, a sensitivity of 82.4%, a specificity of 71.7%, and an AUC of 0.846**.


# 3. Tree-Based Methods


## 3.1 Random Forests


### 3.1.1 Fit random forests model


I further fit a random forests model to predict mortality status. The predictor numbers used in the model is $\sqrt{37-1}=6$.


```{r}
# specify recipe
rf_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric())
```


```{r}
# specify random forests model
rf_spec <- rand_forest(mtry = 6, trees = 1000, min_n = tune()) %>%
    set_engine("ranger") %>%
    set_mode("classification")
```


```{r}
# build random forests model workflow
rf_workflow <- workflow() %>% 
    add_model(rf_spec) %>% 
    add_recipe(rf_recipe)
```


The best tuning parameter `min_n` (the number of observations needed to keep splitting nodes) is chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
# tuning parameter
set.seed(345)
tune_res <- tune_grid(
    rf_workflow,
    resamples = data_fold,
    grid = 20
)

tune_res %>% autoplot() + theme_minimal()
```


```{r}
best_min_n <- select_best(tune_res, "roc_auc")
best_min_n %>% kable()
```


I then re-fit the random forests model using the whole training set.


```{r}
rf_final <- finalize_workflow(rf_workflow, best_min_n)

rf_final_fit <- rf_final %>% fit(data = data_train)
rf_final_fit
```


Below shows the variance importance plot of predictors.


```{r}
rf_final <- finalize_model(rf_spec, best_min_n)
rf_final %>%
    set_engine("ranger", importance = "permutation") %>%
    fit(mortstat ~ ., data = data_train) %>%
    vip(geom = "col") +
    theme_minimal()
```


### 3.1.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_preds <- rf_final_fit %>%
    augment(new_data = data_test)
```


```{r}
data_preds %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an **accuracy of 82.9%** and an **AUC of 0.864**, a relative high **sensitivity of 95.7%**, but a low **specificity of 32.1%**.


### 3.1.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3 and 2.2.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- rf_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest j_index.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and j_index across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold) %>% 
    kable()
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- rf_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
data_preds_rf <- data_preds_final
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, **after balancing sensitivity and specificity**, the model results in **an accuracy of 81.7%, a sensitivity of 84.3%, a specificity of 71.7%, and an AUC of 0.864**.


## 3.2 Boosting trees


### 3.2.1 Fit boosting trees model


I further try boosting trees model to increase the prediction performance.


```{r}
# specify recipe
xgb_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric())
```


```{r}
# specify boosting trees model
xgb_spec <- boost_tree(trees = 1000, 
                       tree_depth = tune(), min_n = tune(), 
                       loss_reduction = tune(),
                       sample_size = tune(), mtry = tune(),
                       learn_rate = tune()) %>% 
    set_engine("xgboost") %>% 
    set_mode("classification")
```


```{r}
# build boosting trees workflow
xgb_workflow <- workflow() %>%
    add_model(xgb_spec) %>% 
    add_recipe(xgb_recipe)
```


Below code specify range of possible values for tuning parameters.


```{r}
# tuning parameters
xgb_grid <- grid_latin_hypercube(tree_depth(),
                                 min_n(),
                                 loss_reduction(),
                                 sample_size = sample_prop(),
                                 finalize(mtry(), data_train),
                                 learn_rate(),
                                 size = 30)
```


The best tuning parameters are chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
doParallel::registerDoParallel()

set.seed(456)
xgb_res <- tune_grid(xgb_workflow,
                     resamples = data_fold,
                     grid = xgb_grid,
                     control = control_grid(save_pred = TRUE))

xgb_res %>% autoplot() + theme_minimal()
```


```{r}
best_auc <- select_best(xgb_res, "roc_auc")
best_auc %>% kable()
```


I then re-fit the boosting trees model using the whole training set.


```{r}
xgb_final <- finalize_workflow(xgb_workflow, best_auc)

xgb_final_fit <- fit(xgb_final, data = data_train)
xgb_final_fit
```


Below shows the variance importance plot of predictors.


```{r}
xgb_final <- finalize_model(xgb_spec, best_auc)
xgb_final %>%
    set_engine("xgboost") %>%
    fit(mortstat ~ ., data = data_train) %>%
    vip(geom = "col") +
    theme_minimal()
```


### 3.2.2 Evaluate model performance


The fitted model's performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_preds <- xgb_final_fit %>%
    augment(new_data = data_test)
```


```{r}
data_preds %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an **accuracy of 82.9%** and an **AUC of 0.8648**, a relative high **sensitivity of 96.2%**, but a low **specificity of 30.2%**.


### 3.2.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3, 2.2.3, and 3.1.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- xgb_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest j_index.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and j_index across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold) %>% 
    kable()
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- xgb_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
data_preds_xgb <- data_preds_final
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, **after balancing sensitivity and specificity**, the model results in **an accuracy of 77.9%, a sensitivity of 76.6%, a specificity of 83.0%, and an AUC of 0.864**.


# 4. Support vector machines


I further try support vector machines for classification. I tried a radial basis kernel with tuning parameters chosen by 10-fold cross-validation; however, the model does not seem to perform well. Below will only report the results of polynomial/linear basis kernel support vector machines.


### 4.1 Fit polynomial basis kernel SVM model


```{r}
# specify recipe
svm_poly_recipe <- recipe(mortstat ~ ., data = data_train) %>% 
    step_dummy(all_nominal(), - all_outcomes()) %>%
    step_zv(all_numeric())
```


```{r}
# specify polynomial basis kernel SVM model
svm_poly_spec <- svm_poly(cost = tune(), degree = tune()) %>%
    set_mode("classification") %>%
    set_engine("kernlab")
```


```{r}
# build polynomial basis kernel SVM workflow
svm_poly_workflow <- workflow() %>% 
    add_model(svm_poly_spec) %>% 
    add_recipe(svm_poly_recipe)
```


The optimal tuning parameters `cost` and `degree` are chosen using 10-fold cross-validation, according to the AUC metric.


```{r}
param_grid <- grid_regular(cost(), degree(), levels = 10)

set.seed(567)
tune_res <- tune_grid(
    svm_poly_workflow,
    resamples = data_fold,
    grid = param_grid
)

tune_res %>% autoplot() + theme_minimal()
```


```{r}
best_auc <- select_best(tune_res, "roc_auc")
best_auc %>% kable()
```


As shown the output above, the 10-fold cross-validation choose a linear kernel (support vector classifier).


I then re-fit the support vector classifier model using the whole training set.


```{r}
svm_poly_final <- finalize_workflow(svm_poly_workflow, best_auc)

svm_poly_final_fit <- fit(svm_poly_final, data = data_train)
svm_poly_final_fit
```


### 4.2 Evaluate model performance


The fitted model’s performance is estimated in the testing set. The threshold for classification is a predicted probability higher than 50%. Below shows the confusion matrix.


```{r}
data_preds <- svm_poly_final_fit %>%
    augment(new_data = data_test)
```


```{r}
data_preds %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and area under the ROC curve (AUC).


```{r}
data_preds %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, the model results in an **accuracy of 82.9%** and an **AUC of 0.824**, a relative high **sensitivity of 98.1%**, but a low **specificity of 22.6%**.


### 4.3 Model post-processing


Again, the results above indicate that the model lead to a high false negative; thus, the threshold of a predicted probability of 50% for classification may not be the best choice. I further balance sensitivity and specificity using different predicted probability threshold and select best threshold based on same procedures in 2.1.3 and 2.2.3.


I first calculate sensitivity, specificity, and j_index using different predicted probability threshold.


```{r}
data_threshold <- svm_poly_final_fit %>%
    augment(new_data = data_test) %>% 
    threshold_perf(mortstat, `.pred_Assumed alive`, thresholds = seq(0.5, 1, by = 0.0025)) %>%
    filter(.metric != "distance") %>%
    mutate(group = case_when(.metric == "sens" | .metric == "spec" ~ "1",
                             TRUE ~ "2"))
```


I then select the predicted probability threshold with highest j_index.


```{r}
max_j_index_threshold <- data_threshold %>%
    filter(.metric == "j_index") %>%
    filter(.estimate == max(.estimate)) %>%
    pull(.threshold)
```


Below shows the values of sensitivity, specificity, and j_index across different predicted probability threshold.


```{r}
ggplot(data_threshold, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +
    geom_line() +
    scale_color_viridis_d(end = 0.9) +
    scale_alpha_manual(values = c(.4, 1), guide = "none") +
    geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = "grey30") +
    labs(x = "'Good' Threshold\n(above this value is considered 'good')",
         y = "Metric Estimate",
         title = "Balancing performance by varying the threshold",
         subtitle = "Vertical line = Max J-Index") +
    theme_minimal()
```


The best predicted probability threshold selected is shown below.


```{r}
data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>% 
    arrange(.threshold) %>% 
    kable()
```


```{r}
best_threshold <- data_threshold %>% 
    filter(.threshold %in% max_j_index_threshold) %>%
    filter(.metric == "j_index") %>% 
    pull(.threshold)
best_threshold
```


I then re-classify the mortality status in the testing set using the best predicted probability threshold selected.


```{r}
data_preds_final <- svm_poly_final_fit %>%
    augment(new_data = data_test) %>% 
    mutate(.pred_class = make_two_class_pred(`.pred_Assumed alive`, levels(mortstat), 
                                             threshold = best_threshold))
data_preds_svm <- data_preds_final
```


Below shows the confusion matrix using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    conf_mat(truth = mortstat, estimate = .pred_class)
```


Below shows the accuracy, sensitivity, specificity, and AUC, using the best predicted probability threshold selected, balancing sensitivity and specificity.


```{r}
data_preds_final %>% 
    accuracy(truth = mortstat, estimate = .pred_class) %>% 
    bind_rows(data_preds_final %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
    bind_rows(data_preds_final %>% roc_auc(truth = mortstat, `.pred_Assumed alive`)) %>% 
    kable()
```


As shown from the output above, **after balancing sensitivity and specificity**, the model results in **an accuracy of 75.3%, a sensitivity of 75.2%, a specificity of 75.5%, and an AUC of 0.824**.


# 5. Summary


In this final project, I try these 5 models: logistic regression, logistic regression with feature selection penalty (lasso regression), random forests, boosting trees, and support vector machines. **Using a threshold for classification as a predicted probability higher than 50%, the accuracy, sensitivity, specificity, and AUC for each model are as follow.**


```{r}
pred_metr <- function(dataset) {
    dataset %>% 
        accuracy(truth = mortstat, estimate = .pred_class) %>% 
        bind_rows(dataset %>% sens(truth = mortstat, estimate = .pred_class)) %>% 
        bind_rows(dataset %>% spec(truth = mortstat, estimate = .pred_class)) %>% 
        bind_rows(dataset %>% roc_auc(truth = mortstat, `.pred_Assumed alive`))
} 

pred_metric <- list(logistic_fit %>% augment(new_data = data_test),
     lasso_final_fit %>% augment(new_data = data_test),
     rf_final_fit %>% augment(new_data = data_test),
     xgb_final_fit %>% augment(new_data = data_test),
     svm_poly_final_fit %>% augment(new_data = data_test)) %>% 
    map2_dfr(c("logistic", "lasso", "random forest", "boosting", "svm"),
             ~ pred_metr(.x) %>% 
                 mutate(`.model` = .y))
pred_metric %>% arrange(.metric) %>% kable()
```


```{r}
pred_metric %>% 
    mutate(.metric = factor(.metric, levels = c("accuracy", "sens", "spec", "roc_auc"))) %>% 
    mutate(.model = factor(.model, levels = c("logistic", "lasso", "random forest",
                                              "boosting", "svm"))) %>% 
    ggplot(aes(x = .model, y = .estimate, fill = .model)) +
    geom_bar(stat="identity") +
    facet_grid(. ~ .metric) +
    scale_fill_manual(values = c("#fc636b", "#ffb900", "#3be8b0", "#1aafd0", "#6a67ce")) +
    theme(panel.grid = element_line(colour = "grey92"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
          axis.ticks = element_blank(), legend.background = element_blank(), 
          legend.key = element_blank(), panel.background = element_blank(), 
          legend.position="bottom",
          panel.border = element_blank(), strip.background = element_blank(), 
          plot.background = element_blank(), complete = TRUE)
```


As shown in the output above, **all models have a low specificity using this threshold**, indicating a high false negative. **After balancing sensitivity and specificity according to the $j_index$, the accuracy, sensitivity, specificity, and AUC for each model are as follow.**


```{r}
pred_metric_balance <- list(data_preds_logistic, data_preds_lasso, data_preds_rf, data_preds_xgb, data_preds_svm) %>% 
    map2_dfr(c("logistic", "lasso", "random forest", "boosting", "svm"),
             ~ pred_metr(.x) %>% 
                 mutate(`.model` = .y))   # balancing sens and spec
pred_metric_balance %>% arrange(.metric) %>% kable()
```


```{r}
pred_metric_balance %>% 
    mutate(.metric = factor(.metric, levels = c("accuracy", "sens", "spec", "roc_auc"))) %>% 
    mutate(.model = factor(.model, levels = c("logistic", "lasso", "random forest",
                                              "boosting", "svm"))) %>% 
    ggplot(aes(x = .model, y = .estimate, fill = .model)) +
    geom_bar(stat="identity") +
    facet_grid(. ~ .metric) +
    scale_fill_manual(values = c("#fc636b", "#ffb900", "#3be8b0", "#1aafd0", "#6a67ce")) +
    theme(panel.grid = element_line(colour = "grey92"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
          axis.ticks = element_blank(), legend.background = element_blank(), 
          legend.key = element_blank(), panel.background = element_blank(), 
          legend.position="bottom",
          panel.border = element_blank(), strip.background = element_blank(), 
          plot.background = element_blank(), complete = TRUE)
```


The **ROC curve** combining all models is as follow.


```{r}
logistic_roc <- data_preds_logistic %>% 
    roc_curve(truth = mortstat, estimate = `.pred_Assumed alive`) %>% 
    mutate(model = "logistic")

lasso_roc <- data_preds_lasso %>% 
    roc_curve(truth = mortstat, estimate = `.pred_Assumed alive`) %>% 
        mutate(model = "lasso")

rf_roc <- data_preds_rf %>% 
    roc_curve(truth = mortstat, estimate = `.pred_Assumed alive`) %>% 
    mutate(model = "random forest")

xgb_roc <- data_preds_xgb %>% 
    roc_curve(truth = mortstat, estimate = `.pred_Assumed alive`) %>% 
    mutate(model = "boosting")

svm_roc <- data_preds_svm %>% 
    roc_curve(truth = mortstat, estimate = `.pred_Assumed alive`) %>% 
    mutate(model = "svm")

bind_rows(logistic_roc, lasso_roc, rf_roc, xgb_roc, svm_roc) %>% 
    mutate(model = factor(model, levels = c("logistic", "lasso", "random forest",
                                            "boosting", "svm"))) %>% 
    ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) + 
    geom_path(lwd = 1.2, alpha = 0.8) +
    geom_abline(lty = 3) + 
    coord_equal() +
    scale_color_manual(values = c("#fc636b", "#ffb900", "#3be8b0", "#1aafd0", "#6a67ce")) +
    theme(panel.grid = element_line(colour = "grey92"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
          axis.ticks = element_blank(), legend.background = element_blank(), 
          legend.key = element_blank(), panel.background = element_blank(), 
          legend.position="bottom",
          panel.border = element_blank(), strip.background = element_blank(), 
          plot.background = element_blank(), complete = TRUE)
```


I make the following conclusions:

+ **No model outperform other models in all metrics**

+ The **threshold of predicted probability** for classification impacts the balance of prediction sensitivity and specificity.

  - The threshold may also **depend on the aim of the prediction**.
  
  - If the prediction **aim for identify those at high risk of mortality to aim for interventions**, then **support vector machine** may be the best choice in this setting, since it has the highest sensitivity under a threshold of predicted probability greater than 50%.
  
  - If the prediction **aim for identify those at lower risk of mortality**, then **boosting trees** may be the best choice in this setting, since it has the highest specificity after choosing the threshold by balancing sensitivity and specificity.
  
  - Generally, identifying those at high risk of mortality may be of interests to decision makers in clinical settings, thus **sensitivity may make more sense**.
  
+ Despite the threshold of sensitivity and specificity, **AUC is an indication of the discrimination** of the model, and does not rely on the threshold chosen. Based on AUC, **random forests** and **boosting trees** may be the best model to choose.

+ **Lasso regression** still has a relatively good performance, and **if interpretability is of importance in the aim of the prediction**, it may be the best model to choose.

+ In summary, **no model outperform other models in all metrics, and model selection should depend on the aim of the prediction**. Nevertheless, considering the performance in all metrics used, I may choose **boosting trees** as the optimal model.


There are several limitations in my predictions:

+ **All observations with missing values are dropped**. Using mean value imputation or multiple imputation may help to improve the performance of the prediction.

+ **Model calibration is not evaluated.**

+ **Only accuracy, sensitivity, specificity, and AUC are considered** to evaluate the prediction performance.

+ **No literature review or conceptual framework are considered** in building the models.

+ **There may be better ways to select the threshold for predicted probability for classification.**

